{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda2/envs/tensor/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import contextlib\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "\n",
    "TRAINING_PROPORTION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def record_time():\n",
    "    start_time = datetime.datetime.now()\n",
    "    yield\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"Time taken: {result}\".format(result=end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = b'labels'\n",
    "DATA = b'data'\n",
    "FILENAMES = b'filenames'\n",
    "\n",
    "def merge_batches(batches):\n",
    "    return {LABELS: np.array(list(itertools.chain.from_iterable([s[LABELS] for s in batches]))),\n",
    "           DATA: np.array(np.concatenate([s[DATA] for s in batches]), dtype=np.float16),\n",
    "           FILENAMES: list(itertools.chain.from_iterable([s[FILENAMES] for s in batches]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sets = [unpickle('cifar-10-batches-py/data_batch_1'), \n",
    "                 unpickle('cifar-10-batches-py/data_batch_2'),\n",
    "                 unpickle('cifar-10-batches-py/data_batch_3'),\n",
    "                 unpickle('cifar-10-batches-py/data_batch_4'),\n",
    "                 unpickle('cifar-10-batches-py/data_batch_5')]\n",
    "test_sets = [unpickle('cifar-10-batches-py/test_batch')]\n",
    "\n",
    "training = merge_batches(training_sets)\n",
    "test = merge_batches(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 8, 5, 1, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[LABELS][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node {\n",
      "  name: \"MatMul\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\000\\000\\260A\\000\\000\\340A\\000\\000DB\\000\\000\\200B\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"MatMul/_0\"\n",
      "  op: \"_Send\"\n",
      "  input: \"MatMul\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"client_terminated\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"recv_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device_incarnation\"\n",
      "    value {\n",
      "      i: 1\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"tensor_name\"\n",
      "    value {\n",
      "      s: \"edge_4_MatMul\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "library {\n",
      "}\n",
      "versions {\n",
      "  producer: 24\n",
      "}\n",
      ", node {\n",
      "  name: \"MatMul/_1\"\n",
      "  op: \"_Recv\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"client_terminated\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"recv_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device_incarnation\"\n",
      "    value {\n",
      "      i: 1\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"tensor_name\"\n",
      "    value {\n",
      "      s: \"edge_4_MatMul\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"tensor_type\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"_retval_MatMul_0_0\"\n",
      "  op: \"_Retval\"\n",
      "  input: \"MatMul/_1\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"index\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "library {\n",
      "}\n",
      "versions {\n",
      "  producer: 24\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Runs the op.\n",
    "options = tf.RunOptions(output_partition_graphs=True)\n",
    "metadata = tf.RunMetadata()\n",
    "c_val = sess.run(c, options=options, run_metadata=metadata)\n",
    "\n",
    "print(metadata.partition_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'convnet', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc9e343d68>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from convnet/model.ckpt-100\n",
      "INFO:tensorflow:Saving checkpoints for 101 into convnet/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3457, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 200 into convnet/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.3008.\n",
      "Time taken: 0:00:05.073494\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-09-13:54:45\n",
      "INFO:tensorflow:Restoring parameters from convnet/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-09-13:54:45\n",
      "INFO:tensorflow:Saving dict for global step 200: auc = 0.1123, global_step = 200, loss = 2.31997, precision = 0.897796, recall = 0.988315\n",
      "Time taken: 0:00:00.569585\n"
     ]
    }
   ],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 32, 32, 3])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 8 * 8 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"auc\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"precision\": tf.metrics.precision(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"recall\": tf.metrics.recall(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "cifar_cfr = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir='convnet')\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=10)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': training[DATA]},\n",
    "    y = training[LABELS],\n",
    "    batch_size = 512,\n",
    "    num_epochs = 10,\n",
    "    shuffle = True)\n",
    "\n",
    "with record_time():\n",
    "    cifar_cfr.train(train_fn, steps=100)\n",
    "    \n",
    "test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test[DATA]},\n",
    "    y = test[LABELS],\n",
    "    num_epochs = 1,\n",
    "    shuffle = False)\n",
    "\n",
    "with record_time():\n",
    "    evaluation_results = cifar_cfr.evaluate(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5c6d81c07f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'P' is not defined"
     ]
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'alexnet', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc7ac92b38>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into alexnet/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.584, step = 1\n",
      "INFO:tensorflow:global_step/sec: 17.4862\n",
      "INFO:tensorflow:loss = 2.1504, step = 101 (5.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0161\n",
      "INFO:tensorflow:loss = 1.9922, step = 201 (5.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0235\n",
      "INFO:tensorflow:loss = 1.9453, step = 301 (5.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0408\n",
      "INFO:tensorflow:loss = 1.9746, step = 401 (5.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0799\n",
      "INFO:tensorflow:loss = 1.9795, step = 501 (5.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0196\n",
      "INFO:tensorflow:loss = 2.0039, step = 601 (5.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.9345\n",
      "INFO:tensorflow:loss = 1.918, step = 701 (5.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.7994\n",
      "INFO:tensorflow:loss = 2.0605, step = 801 (5.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.857\n",
      "INFO:tensorflow:loss = 1.9189, step = 901 (5.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.7843\n",
      "INFO:tensorflow:loss = 1.9531, step = 1001 (5.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.5077\n",
      "INFO:tensorflow:loss = 1.9492, step = 1101 (5.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.4043\n",
      "INFO:tensorflow:loss = 1.8564, step = 1201 (5.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.6268\n",
      "INFO:tensorflow:loss = 1.8301, step = 1301 (5.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.7723\n",
      "INFO:tensorflow:loss = 1.8779, step = 1401 (5.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0367\n",
      "INFO:tensorflow:loss = 1.8213, step = 1501 (5.545 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into alexnet/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.7871.\n",
      "Time taken: 0:01:28.568141\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-09-15:27:25\n",
      "INFO:tensorflow:Restoring parameters from alexnet/model.ckpt-1563\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-09-15:27:27\n",
      "INFO:tensorflow:Saving dict for global step 1563: auc = 0.3565, global_step = 1563, loss = 1.81566, precision = 0.929161, recall = 0.951703\n",
      "Time taken: 0:00:02.102575\n"
     ]
    }
   ],
   "source": [
    "def alexnet_model_fn(features, labels, mode):\n",
    "    data = features['x']\n",
    "    \n",
    "    input_layer = tf.reshape(data, [-1, 32, 32, 3])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=96,\n",
    "        kernel_size=11,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    norm1 = tf.contrib.layers.layer_norm(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=norm1, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv2 = tf.layers.conv2d(\n",
    "#         inputs=pool1,\n",
    "#         filters=256,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     norm2 = tf.contrib.layers.layer_norm(conv2)\n",
    "#     pool2 = tf.layers.max_pooling2d(inputs=norm2, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv3 = tf.layers.conv2d(\n",
    "#         inputs=pool2,\n",
    "#         filters=384,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv4 = tf.layers.conv2d(\n",
    "#         inputs=pool3,\n",
    "#         filters=384,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv5 = tf.layers.conv2d(\n",
    "#         inputs=pool4,\n",
    "#         filters=256,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=2, strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    flat = tf.reshape(pool1, [-1, 96 * 16 * 16])\n",
    "    dense1 = tf.layers.dense(inputs=flat, units=2048, activation=tf.nn.relu)\n",
    "    dropout1 = tf.layers.dropout(\n",
    "        inputs=dense1, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dense2 = tf.layers.dense(inputs=dropout1, units=2048, activation=tf.nn.relu)\n",
    "    dropout2 = tf.layers.dropout(\n",
    "        inputs=dense2, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout2, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"auc\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"precision\": tf.metrics.precision(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"recall\": tf.metrics.recall(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "directory = 'alexnet'\n",
    "try:\n",
    "    os.removedirs(directory)\n",
    "except OSError:\n",
    "    pass\n",
    "alex_cfr = tf.estimator.Estimator(model_fn=alexnet_model_fn, model_dir=directory)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=10)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': training[DATA]},\n",
    "    y = training[LABELS],\n",
    "    num_epochs = 5,\n",
    "    batch_size=128,\n",
    "    shuffle = True)\n",
    "\n",
    "with record_time():\n",
    "    alex_cfr.train(train_fn, steps=None)\n",
    "    \n",
    "test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test[DATA]},\n",
    "    y = test[LABELS],\n",
    "    num_epochs = 1,\n",
    "    shuffle = True)\n",
    "with record_time():\n",
    "    evaluation_results = alex_cfr.evaluate(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'zfnet', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc7a3f9a90>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[338,10] labels_size=[128,10]\n\t [[Node: softmax_cross_entropy_loss/xentropy = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_loss/Reshape, softmax_cross_entropy_loss/Reshape_1)]]\n\t [[Node: GradientDescent/update/_130 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1010_GradientDescent/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'softmax_cross_entropy_loss/xentropy', defined at:\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-63-932af4135efd>\", line 110, in <module>\n    zf_cfr.train(train_fn, steps=None)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"<ipython-input-63-932af4135efd>\", line 71, in zfnet_model_fn\n    onehot_labels=onehot_labels, logits=logits)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\", line 683, in softmax_cross_entropy\n    name=\"xentropy\")\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[338,10] labels_size=[128,10]\n\t [[Node: softmax_cross_entropy_loss/xentropy = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_loss/Reshape, softmax_cross_entropy_loss/Reshape_1)]]\n\t [[Node: GradientDescent/update/_130 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1010_GradientDescent/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[338,10] labels_size=[128,10]\n\t [[Node: softmax_cross_entropy_loss/xentropy = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_loss/Reshape, softmax_cross_entropy_loss/Reshape_1)]]\n\t [[Node: GradientDescent/update/_130 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1010_GradientDescent/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-932af4135efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrecord_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mzf_cfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m test_fn = tf.estimator.inputs.numpy_input_fn(\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    890\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    893\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[338,10] labels_size=[128,10]\n\t [[Node: softmax_cross_entropy_loss/xentropy = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_loss/Reshape, softmax_cross_entropy_loss/Reshape_1)]]\n\t [[Node: GradientDescent/update/_130 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1010_GradientDescent/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'softmax_cross_entropy_loss/xentropy', defined at:\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-63-932af4135efd>\", line 110, in <module>\n    zf_cfr.train(train_fn, steps=None)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"<ipython-input-63-932af4135efd>\", line 71, in zfnet_model_fn\n    onehot_labels=onehot_labels, logits=logits)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\", line 683, in softmax_cross_entropy\n    name=\"xentropy\")\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/will/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[338,10] labels_size=[128,10]\n\t [[Node: softmax_cross_entropy_loss/xentropy = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_loss/Reshape, softmax_cross_entropy_loss/Reshape_1)]]\n\t [[Node: GradientDescent/update/_130 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1010_GradientDescent/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "def zfnet_model_fn(features, labels, mode):\n",
    "    data = features['x']\n",
    "    \n",
    "    input_layer = tf.reshape(data, [-1, 32, 32, 3])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=96,\n",
    "        kernel_size=7,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    norm1 = tf.contrib.layers.layer_norm(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=norm1, pool_size=3, strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=256,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    norm2 = tf.contrib.layers.layer_norm(conv2)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=norm2, pool_size=3, strides=1)\n",
    "    \n",
    "#     conv3 = tf.layers.conv2d(\n",
    "#         inputs=pool2,\n",
    "#         filters=384,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=3, strides=1)\n",
    "    \n",
    "#     conv4 = tf.layers.conv2d(\n",
    "#         inputs=pool3,\n",
    "#         filters=384,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=3, strides=1)\n",
    "    \n",
    "#     conv5 = tf.layers.conv2d(\n",
    "#         inputs=pool4,\n",
    "#         filters=256,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=3, strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    flat = tf.reshape(pool2, [-1, 256 * 8 * 8])\n",
    "    dense1 = tf.layers.dense(inputs=flat, units=2048, activation=tf.nn.relu)\n",
    "    dropout1 = tf.layers.dropout(\n",
    "        inputs=dense1, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dense2 = tf.layers.dense(inputs=dropout1, units=2048, activation=tf.nn.relu)\n",
    "    dropout2 = tf.layers.dropout(\n",
    "        inputs=dense2, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout2, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"auc\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"precision\": tf.metrics.precision(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"recall\": tf.metrics.recall(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "directory = 'zfnet'\n",
    "try:\n",
    "    os.removedirs(directory)\n",
    "except OSError:\n",
    "    pass\n",
    "zf_cfr = tf.estimator.Estimator(model_fn=zfnet_model_fn, model_dir=directory)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=10)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': training[DATA]},\n",
    "    y = training[LABELS],\n",
    "    num_epochs = 5,\n",
    "    batch_size=128,\n",
    "    shuffle = True)\n",
    "\n",
    "with record_time():\n",
    "    zf_cfr.train(train_fn, steps=None)\n",
    "    \n",
    "test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test[DATA]},\n",
    "    y = test[LABELS],\n",
    "    num_epochs = 1,\n",
    "    shuffle = True)\n",
    "with record_time():\n",
    "    evaluation_results = zf_cfr.evaluate(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'vggnet', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc7ab87fd0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into vggnet/model.ckpt.\n",
      "INFO:tensorflow:loss = 37.469, step = 1\n",
      "INFO:tensorflow:global_step/sec: 21.2935\n",
      "INFO:tensorflow:loss = 2.3027, step = 101 (4.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.6004\n",
      "INFO:tensorflow:loss = 2.3223, step = 201 (4.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.2194\n",
      "INFO:tensorflow:loss = 2.2734, step = 301 (4.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.0702\n",
      "INFO:tensorflow:loss = 2.293, step = 401 (4.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.1337\n",
      "INFO:tensorflow:loss = 2.2949, step = 501 (4.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.3548\n",
      "INFO:tensorflow:loss = 2.2734, step = 601 (4.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.9889\n",
      "INFO:tensorflow:loss = 2.2949, step = 701 (4.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.0657\n",
      "INFO:tensorflow:loss = 2.3438, step = 801 (4.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.8582\n",
      "INFO:tensorflow:loss = 2.3105, step = 901 (4.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.9945\n",
      "INFO:tensorflow:loss = 2.3047, step = 1001 (4.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.0119\n",
      "INFO:tensorflow:loss = 2.3145, step = 1101 (4.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.0428\n",
      "INFO:tensorflow:loss = 2.2754, step = 1201 (4.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.0888\n",
      "INFO:tensorflow:loss = 2.25, step = 1301 (4.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.045\n",
      "INFO:tensorflow:loss = 2.2578, step = 1401 (4.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.9528\n",
      "INFO:tensorflow:loss = 2.2598, step = 1501 (4.555 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into vggnet/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.1973.\n",
      "Time taken: 0:01:12.149859\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-09-15:13:11\n",
      "INFO:tensorflow:Restoring parameters from vggnet/model.ckpt-1563\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-09-15:13:12\n",
      "INFO:tensorflow:Saving dict for global step 1563: auc = 0.176, global_step = 1563, loss = 2.19754, precision = 0.935091, recall = 0.461718\n",
      "Time taken: 0:00:01.643205\n"
     ]
    }
   ],
   "source": [
    "def vggnet_model_fn(features, labels, mode):\n",
    "    data = features['x']\n",
    "    \n",
    "    input_layer = tf.reshape(data, [-1, 32, 32, 3])\n",
    "    \n",
    "    conv1a = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=conv1a,\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv2a = tf.layers.conv2d(\n",
    "#         inputs=pool1,\n",
    "#         filters=128,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv2 = tf.layers.conv2d(\n",
    "#         inputs=conv2a,\n",
    "#         filters=128,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv3a = tf.layers.conv2d(\n",
    "#         inputs=pool2,\n",
    "#         filters=256,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv3b = tf.layers.conv2d(\n",
    "#         inputs=conv3a,\n",
    "#         filters=256,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv3 = tf.layers.conv2d(\n",
    "#         inputs=conv3b,\n",
    "#         filters=256,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv4a = tf.layers.conv2d(\n",
    "#         inputs=pool3,\n",
    "#         filters=512,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv4b = tf.layers.conv2d(\n",
    "#         inputs=conv4a,\n",
    "#         filters=512,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv4 = tf.layers.conv2d(\n",
    "#         inputs=conv4b,\n",
    "#         filters=512,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=2, strides=2)\n",
    "    \n",
    "#     conv5a = tf.layers.conv2d(\n",
    "#         inputs=pool4,\n",
    "#         filters=512,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv5b = tf.layers.conv2d(\n",
    "#         inputs=conv5a,\n",
    "#         filters=512,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     conv5 = tf.layers.conv2d(\n",
    "#         inputs=conv5b,\n",
    "#         filters=512,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=2, strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    flat = tf.reshape(pool1, [-1, 64 * 16 * 16])\n",
    "    dense1 = tf.layers.dense(inputs=flat, units=4096, activation=tf.nn.relu)\n",
    "    dropout1 = tf.layers.dropout(\n",
    "        inputs=dense1, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dense2 = tf.layers.dense(inputs=dropout1, units=4096, activation=tf.nn.relu)\n",
    "    dropout2 = tf.layers.dropout(\n",
    "        inputs=dense2, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dense3 = tf.layers.dense(inputs=dropout2, units=1000, activation=tf.nn.relu)\n",
    "    dropout3 = tf.layers.dropout(\n",
    "        inputs=dense3, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout3, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"auc\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"precision\": tf.metrics.precision(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"recall\": tf.metrics.recall(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "directory = 'vggnet'\n",
    "try:\n",
    "    os.removedirs(directory)\n",
    "except OSError:\n",
    "    pass\n",
    "vgg_cfr = tf.estimator.Estimator(model_fn=vggnet_model_fn, model_dir=directory)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=10)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': training[DATA]},\n",
    "    y = training[LABELS],\n",
    "    num_epochs = 5,\n",
    "    batch_size=128,\n",
    "    shuffle = True)\n",
    "\n",
    "with record_time():\n",
    "    vgg_cfr.train(train_fn, steps=None)\n",
    "    \n",
    "test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test[DATA]},\n",
    "    y = test[LABELS],\n",
    "    num_epochs = 1,\n",
    "    shuffle = True)\n",
    "with record_time():\n",
    "    evaluation_results = vgg_cfr.evaluate(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'googlenet', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc7a1fe198>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into googlenet/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3457, step = 1\n",
      "INFO:tensorflow:global_step/sec: 33.1805\n",
      "INFO:tensorflow:loss = 2.3086, step = 101 (3.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.9312\n",
      "INFO:tensorflow:loss = 2.3379, step = 201 (2.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.1772\n",
      "INFO:tensorflow:loss = 2.2988, step = 301 (2.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.0742\n",
      "INFO:tensorflow:loss = 2.3027, step = 401 (2.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8678\n",
      "INFO:tensorflow:loss = 2.2734, step = 501 (2.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9763\n",
      "INFO:tensorflow:loss = 2.3066, step = 601 (2.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9963\n",
      "INFO:tensorflow:loss = 2.293, step = 701 (2.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9347\n",
      "INFO:tensorflow:loss = 2.2656, step = 801 (2.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8488\n",
      "INFO:tensorflow:loss = 2.2793, step = 901 (2.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.0541\n",
      "INFO:tensorflow:loss = 2.2852, step = 1001 (2.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9124\n",
      "INFO:tensorflow:loss = 2.2812, step = 1101 (2.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.226\n",
      "INFO:tensorflow:loss = 2.2988, step = 1201 (2.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8827\n",
      "INFO:tensorflow:loss = 2.2734, step = 1301 (2.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6839\n",
      "INFO:tensorflow:loss = 2.2656, step = 1401 (2.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.381\n",
      "INFO:tensorflow:loss = 2.2871, step = 1501 (2.820 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1563 into googlenet/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.2539.\n",
      "Time taken: 0:00:46.127003\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-09-16:16:23\n",
      "INFO:tensorflow:Restoring parameters from googlenet/model.ckpt-1563\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-09-16:16:24\n",
      "INFO:tensorflow:Saving dict for global step 1563: auc = 0.2342, global_step = 1563, loss = 2.24876, precision = 0.951399, recall = 0.77554\n",
      "Time taken: 0:00:01.743811\n"
     ]
    }
   ],
   "source": [
    "def googlenet_model_fn(features, labels, mode):\n",
    "    data = features['x']\n",
    "    \n",
    "    input_layer = tf.reshape(data, [-1, 32, 32, 3])\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv1, \n",
    "        pool_size=3, \n",
    "        strides=2,\n",
    "        padding=\"same\")\n",
    "    \n",
    "    norm1 = tf.contrib.layers.layer_norm(pool1)\n",
    "    \n",
    "    conv2a = tf.layers.conv2d(\n",
    "        inputs=norm1,\n",
    "        filters=64,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=conv2a,\n",
    "        filters=192,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    norm2 = tf.contrib.layers.layer_norm(conv2)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs=norm2, \n",
    "        pool_size=3, \n",
    "        strides=2,\n",
    "        padding=\"same\")\n",
    "    \n",
    "    incept_1_1 = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=64,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_1_3a = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=96,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_1_3 = tf.layers.conv2d(\n",
    "        inputs=incept_1_3a,\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_1_5a = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=16,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_1_5 = tf.layers.conv2d(\n",
    "        inputs=incept_1_5a,\n",
    "        filters=32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_1_mpa = tf.layers.max_pooling2d(\n",
    "        inputs=pool2, \n",
    "        padding='same', \n",
    "        pool_size=3, \n",
    "        strides=1)\n",
    "    incept_1_mp = tf.layers.conv2d(\n",
    "        inputs=incept_1_mpa,\n",
    "        filters=32,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_1_p = tf.concat(values=[incept_1_1, incept_1_3, incept_1_5, incept_1_mp], axis=3)\n",
    "    \n",
    "    incept_2_1 = tf.layers.conv2d(\n",
    "        inputs=incept_1_p,\n",
    "        filters=128,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_2_3a = tf.layers.conv2d(\n",
    "        inputs=incept_1_p,\n",
    "        filters=128,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_2_3 = tf.layers.conv2d(\n",
    "        inputs=incept_2_3a,\n",
    "        filters=192,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_2_5a = tf.layers.conv2d(\n",
    "        inputs=incept_1_p,\n",
    "        filters=32,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_2_5 = tf.layers.conv2d(\n",
    "        inputs=incept_2_5a,\n",
    "        filters=96,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_2_mpa = tf.layers.max_pooling2d(\n",
    "        inputs=incept_1_p, \n",
    "        padding='same', \n",
    "        pool_size=3, \n",
    "        strides=1)\n",
    "    incept_2_mp = tf.layers.conv2d(\n",
    "        inputs=incept_2_mpa,\n",
    "        filters=64,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_2_p = tf.concat(values=[incept_2_1, incept_2_3, incept_2_5, incept_2_mp], axis=3)\n",
    "\n",
    "    pool3 = tf.layers.max_pooling2d(\n",
    "        inputs=incept_2_p, \n",
    "        pool_size=3, \n",
    "        strides=2,\n",
    "        padding='same')\n",
    "    \n",
    "    incept_3_1 = tf.layers.conv2d(\n",
    "        inputs=pool3,\n",
    "        filters=192,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_3_3a = tf.layers.conv2d(\n",
    "        inputs=pool3,\n",
    "        filters=96,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_3_3 = tf.layers.conv2d(\n",
    "        inputs=incept_3_3a,\n",
    "        filters=208,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_3_5a = tf.layers.conv2d(\n",
    "        inputs=pool3,\n",
    "        filters=16,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_3_5 = tf.layers.conv2d(\n",
    "        inputs=incept_3_5a,\n",
    "        filters=48,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_3_mpa = tf.layers.max_pooling2d(\n",
    "        inputs=pool3, \n",
    "        padding='same', \n",
    "        pool_size=3, \n",
    "        strides=1)\n",
    "    incept_3_mp = tf.layers.conv2d(\n",
    "        inputs=incept_3_mpa,\n",
    "        filters=64,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    incept_3_p = tf.concat(values=[incept_3_1, incept_3_3, incept_3_5, incept_3_mp], axis=3)\n",
    "    \n",
    "#     incept_4_1 = tf.layers.conv2d(\n",
    "#         inputs=incept_3_p,\n",
    "#         filters=160,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_4_3a = tf.layers.conv2d(\n",
    "#         inputs=incept_3_p,\n",
    "#         filters=112,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_4_3 = tf.layers.conv2d(\n",
    "#         inputs=incept_4_3a,\n",
    "#         filters=224,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_4_5a = tf.layers.conv2d(\n",
    "#         inputs=incept_3_p,\n",
    "#         filters=24,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_4_5 = tf.layers.conv2d(\n",
    "#         inputs=incept_4_5a,\n",
    "#         filters=64,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_4_mpa = tf.layers.max_pooling2d(\n",
    "#         inputs=incept_3_p, \n",
    "#         padding='same', \n",
    "#         pool_size=3, \n",
    "#         strides=1)\n",
    "#     incept_4_mp = tf.layers.conv2d(\n",
    "#         inputs=incept_4_mpa,\n",
    "#         filters=64,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_4_p = tf.concat(values=[incept_4_1, incept_4_3, incept_4_5, incept_4_mp], axis=3)\n",
    "    \n",
    "#     incept_5_1 = tf.layers.conv2d(\n",
    "#         inputs=incept_4_p,\n",
    "#         filters=128,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_5_3a = tf.layers.conv2d(\n",
    "#         inputs=incept_4_p,\n",
    "#         filters=128,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_5_3 = tf.layers.conv2d(\n",
    "#         inputs=incept_5_3a,\n",
    "#         filters=256,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_5_5a = tf.layers.conv2d(\n",
    "#         inputs=incept_4_p,\n",
    "#         filters=24,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_5_5 = tf.layers.conv2d(\n",
    "#         inputs=incept_5_5a,\n",
    "#         filters=64,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_5_mpa = tf.layers.max_pooling2d(\n",
    "#         inputs=incept_4_p, \n",
    "#         padding='same', \n",
    "#         pool_size=3, \n",
    "#         strides=1)\n",
    "#     incept_5_mp = tf.layers.conv2d(\n",
    "#         inputs=incept_5_mpa,\n",
    "#         filters=64,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_5_p = tf.concat(values=[incept_5_1, incept_5_3, incept_5_5, incept_5_mp], axis=3)\n",
    "    \n",
    "#     incept_6_1 = tf.layers.conv2d(\n",
    "#         inputs=incept_5_p,\n",
    "#         filters=112,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_6_3a = tf.layers.conv2d(\n",
    "#         inputs=incept_5_p,\n",
    "#         filters=144,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_6_3 = tf.layers.conv2d(\n",
    "#         inputs=incept_6_3a,\n",
    "#         filters=288,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_6_5a = tf.layers.conv2d(\n",
    "#         inputs=incept_5_p,\n",
    "#         filters=32,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_6_5 = tf.layers.conv2d(\n",
    "#         inputs=incept_6_5a,\n",
    "#         filters=64,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_6_mpa = tf.layers.max_pooling2d(\n",
    "#         inputs=incept_5_p, \n",
    "#         padding='same', \n",
    "#         pool_size=3, \n",
    "#         strides=1)\n",
    "#     incept_6_mp = tf.layers.conv2d(\n",
    "#         inputs=incept_6_mpa,\n",
    "#         filters=64,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_6_p = tf.concat(values=[incept_6_1, incept_6_3, incept_6_5, incept_6_mp], axis=3)\n",
    "    \n",
    "#     incept_7_1 = tf.layers.conv2d(\n",
    "#         inputs=incept_6_p,\n",
    "#         filters=256,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_7_3a = tf.layers.conv2d(\n",
    "#         inputs=incept_6_p,\n",
    "#         filters=160,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_7_3 = tf.layers.conv2d(\n",
    "#         inputs=incept_7_3a,\n",
    "#         filters=320,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_7_5a = tf.layers.conv2d(\n",
    "#         inputs=incept_6_p,\n",
    "#         filters=32,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_7_5 = tf.layers.conv2d(\n",
    "#         inputs=incept_7_5a,\n",
    "#         filters=128,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_7_mpa = tf.layers.max_pooling2d(\n",
    "#         inputs=incept_6_p, \n",
    "#         padding='same', \n",
    "#         pool_size=3, \n",
    "#         strides=1)\n",
    "#     incept_7_mp = tf.layers.conv2d(\n",
    "#         inputs=incept_7_mpa,\n",
    "#         filters=128,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_7_p = tf.concat(values=[incept_7_1, incept_7_3, incept_7_5, incept_7_mp], axis=3)\n",
    "\n",
    "#     pool4 = tf.layers.max_pooling2d(\n",
    "#         inputs=incept_7_p, \n",
    "#         pool_size=3, \n",
    "#         strides=2,\n",
    "#         padding='same')\n",
    "    \n",
    "#     incept_8_1 = tf.layers.conv2d(\n",
    "#         inputs=pool4,\n",
    "#         filters=256,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_8_3a = tf.layers.conv2d(\n",
    "#         inputs=pool4,\n",
    "#         filters=160,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_8_3 = tf.layers.conv2d(\n",
    "#         inputs=incept_8_3a,\n",
    "#         filters=320,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_8_5a = tf.layers.conv2d(\n",
    "#         inputs=pool4,\n",
    "#         filters=32,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_8_5 = tf.layers.conv2d(\n",
    "#         inputs=incept_8_5a,\n",
    "#         filters=128,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_8_mpa = tf.layers.max_pooling2d(\n",
    "#         inputs=pool4, \n",
    "#         padding='same', \n",
    "#         pool_size=3, \n",
    "#         strides=1)\n",
    "#     incept_8_mp = tf.layers.conv2d(\n",
    "#         inputs=incept_8_mpa,\n",
    "#         filters=128,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_8_p = tf.concat(values=[incept_8_1, incept_8_3, incept_8_5, incept_8_mp], axis=3)\n",
    "    \n",
    "#     incept_9_1 = tf.layers.conv2d(\n",
    "#         inputs=incept_8_p,\n",
    "#         filters=384,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_9_3a = tf.layers.conv2d(\n",
    "#         inputs=incept_8_p,\n",
    "#         filters=192,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_9_3 = tf.layers.conv2d(\n",
    "#         inputs=incept_9_3a,\n",
    "#         filters=384,\n",
    "#         kernel_size=3,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_9_5a = tf.layers.conv2d(\n",
    "#         inputs=incept_8_p,\n",
    "#         filters=48,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_9_5 = tf.layers.conv2d(\n",
    "#         inputs=incept_9_5a,\n",
    "#         filters=128,\n",
    "#         kernel_size=5,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_9_mpa = tf.layers.max_pooling2d(\n",
    "#         inputs=incept_8_p, \n",
    "#         padding='same', \n",
    "#         pool_size=3, \n",
    "#         strides=1)\n",
    "#     incept_9_mp = tf.layers.conv2d(\n",
    "#         inputs=incept_9_mpa,\n",
    "#         filters=128,\n",
    "#         kernel_size=1,\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "#     incept_9_p = tf.concat(values=[incept_9_1, incept_9_3, incept_9_5, incept_9_mp], axis=3)\n",
    "\n",
    "    last_pool = tf.layers.average_pooling2d(\n",
    "        inputs=incept_3_p, \n",
    "        pool_size=5, \n",
    "        strides=1, padding='same')\n",
    "    last_conv = tf.layers.conv2d(\n",
    "        inputs=last_pool,\n",
    "        filters=1024,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    # Dense Layer\n",
    "    flat = tf.reshape(last_conv, [-1, 1024 * 4])\n",
    "    dense1 = tf.layers.dense(inputs=flat, units=1000, activation=tf.nn.relu)\n",
    "    dropout1 = tf.layers.dropout(inputs=dense1, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dense2 = tf.layers.dense(inputs=dropout1, units=1000, activation=tf.nn.relu)\n",
    "    dropout2 = tf.layers.dropout(inputs=dense2, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout2, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"auc\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"precision\": tf.metrics.precision(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"recall\": tf.metrics.recall(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "directory = 'googlenet'\n",
    "try:\n",
    "    os.removedirs(directory)\n",
    "except OSError:\n",
    "    pass\n",
    "google_cfr = tf.estimator.Estimator(model_fn=googlenet_model_fn, model_dir=directory)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=10)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': training[DATA]},\n",
    "    y = training[LABELS],\n",
    "    num_epochs = 5,\n",
    "    batch_size=128,\n",
    "    shuffle = True)\n",
    "\n",
    "with record_time():\n",
    "    google_cfr.train(train_fn, steps=None)\n",
    "    \n",
    "test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test[DATA]},\n",
    "    y = test[LABELS],\n",
    "    num_epochs = 1,\n",
    "    shuffle = True)\n",
    "with record_time():\n",
    "    evaluation_results = google_cfr.evaluate(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
